## 第2章 感知机



## 第3章 k近邻



## 第4章 朴素贝叶斯

朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设得分类法。

```mermaid
graph LR
训练数据--> 联合概率分布
联合概率分布--> 后验概率最大化的输出Y
```

### 4.1 朴素贝叶斯法的学习与分类

1. 基本方法

​	朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y).具体地，学习以下先验概率分布及条件概率分布。

先验概率分布：$P(Y=c_k),k=1,2,..,K$

条件概率分布：$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},..,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K$

朴素贝叶斯法对条件概率分布作了条件独立性假设。

$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},..,X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$

后验概率计算：
$$
\begin{align}
P(Y=c_k|X=x)&=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\
&=\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
\end{align}
$$
朴素贝叶斯分类器：
$$
y=f(x)=argmax_{c_k}\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
注意到，分母对所有的$c_k$都是相同的，所以
$$
y=argmax_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)
$$

2. 后验概率最大化的含义

   朴素贝叶斯法将实例分到**后验概率最大**的类中。这等价于**期望风险最小化**。

> + 选择0-1损失函数
>
>   $L(Y,f(X))=\begin{cases}1,\quad Y\neq f(X)\\0,\quad Y=f(X) \end{cases}$
>
> + 期望风险函数
>
>   $R_{exp}(f)=E[L(Y,f(X))]=E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)$
>
> + 期望风险最小化
>   $$
>   \begin{align}
>   f(x)&=argmin_{y\in Y}\sum_{k=1}^KL(c_k,y)P(c_k|X=x)\\
>   &=argmin_{y\in Y}\sum_{k=1}^KP(y\neq c_K|X=x)\\
>   &=argmin_{y\in Y}\sum_{k=1}^K(1-P(y=c_k|X=x))\\
>   &=argmax_{y\in Y}P(y=c_k|X=x)
>   \end{align}
>   $$

### 4.2 朴素贝叶斯法的参数估计

1. 极大似然估计

   + 先验概率的极大似然估计

     ​	$P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,...,K$

   + 条件概率的极大似然估计

     $P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_jl,y_i=c_k)}{\sum_{i=1}^NI(Y_i=c_k)}$

2. 贝叶斯估计

   > 用极大似然估计可能会出现所要估计的概率值为0的情况。这会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。

   + 条件概率贝叶斯估计
     $$
     P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}\\
     
     l=1,2,...,S_j\\
     k=1,2,...,K
     $$

     > 式中$\lambda\geq0$,常取$\lambda=1$,这时成为拉普拉斯平滑。

   + 先验概率的贝叶斯估计
     $$
     P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}
     $$

3. 学习与分类算法

   1. 计算先验概率及条件概率

   2. 对于给定的实例$X=(x^{(1)},x^{(2),...,x^{(n)}})^T$,计算

      $P(y=c_k)\prod_{j=1}^nP(x^{(j)}=x^{(j)|Y=c_k}),k=1,2,...,K$

   3. 确定实例x的类

      $y=argmax_{c_k}P(y=c_k)\prod_{j=1}^nP(x^{(j)}=x^{(j)|Y=c_k})$

## 第5章 决策树



## 第6章 逻辑斯谛回归与最大熵模型

逻辑斯谛回归模型与最大熵模型都属于对数线性模型。

### 6.1 逻辑斯谛回归模型

1. 逻辑斯谛分布

> 定义：设X是连续型随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：
> $$
> F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}\\
> f(x)=F\prime(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
> $$

2. 二项逻辑斯谛回归模型

> 定义：（逻辑斯谛回归模型）
>
> 二项逻辑斯谛回归模型是如下的条件概率分布：
> $$
> P(Y=1|x)=\frac{exp(w*x+b)}{1+exp(w*x+b)}\\
> P(Y=0|x)=\frac{1}{1+exp(w*x+b)}
> $$
> 为了方便，将权值向量和输入空间进行扩充，可记为：
> $$
> P(Y=1|x)=\frac{exp(w*x)}{1+exp(w*x)}\\
> P(Y=0|x)=\frac{1}{1+exp(w*x)}
> $$
> 

3. 模型参数估计

> 应用极大似然估计法估计模型参数：
>
> 设：$P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)$
>
> 似然函数为：
>
> ​		$\prod_{i=1}^N [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$
>
> 对数似然函数为：
> $$
> \begin{align}
> L(w)&=\sum_{i=1}^N[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]\\
> &=\sum_{i=1}^N[y_ilog{\frac{\pi(x_i)}{1-\pi(x_i)}}+log(1-\pi(x_i))]\\
> &=\sum_{i=1}^N[y_i(w*x_i)-log(1+exp(w*x_i))]
> \end{align}
> $$

4. 多项逻辑斯谛回归

> 假设离散型随机变量Y的取值集合是{1,2,...,K},那么多项逻辑斯谛回归模型是：
> $$
> P(Y=k|x)=\frac{exp(w_k*x)}{1+\sum_{k=1}^{K-1}exp(w_k*x)},k=1,2,...,K-1\\
> P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k*x)}\\
> 这里，x\in R^{n+1},w_k\in R^{n+1}
> $$

### 6.2 最大熵模型

1. 最大熵原理

   最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。

   假设离散随机变量X的概率分布是P(x),则其熵是:

   ​	$H(P)=-\sum_{x}P(x)logP(x)$

   熵满足不等式：

   ​	$0\leq H(P)\leq log|X|$

2. 最大熵模型的定义

   `定义` (**最大熵模型**)

   ​	假设满足所有约束条件的模型集合为：

   ​         $\mathcal{C}\equiv\{P\in \mathcal{P}|E_p(f_i)=E_{\bar P}(f_i),i=1,2,...,n\}$

    	定义在条件概率分布P(Y|X)上的条件熵为：

   ​		$H(P)=-\sum_{x,y}\tilde P(x)P(y|x)logP(y|x)$

   ​	则模型集合$\mathcal C$中条件熵H(P)最大的模型称为最大熵模型。

3. 最大熵模型的学习

   最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题。

   对给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$以及特征函数$f_i(x,y),i=1,2,...,n$,最大熵模型的学习等价于约束最优化问题：
   $$
   \begin{align}
   max_{P\in \mathcal C}\qquad &H(P)=-\sum_{x,y}\tilde P(x)P(y|x)logP(y|x)\\
   s.t. \qquad &E_P(f_i)=E_{\tilde P}(f_i),i=1,2,...,n\\
   &\sum_{y}P(y|x)=1
   \end{align}
   $$

## 第7章 支持向量机

### 7.1 线性可分支持向量机与硬间隔最大化

1. 线性可分支持向量机

   `定义` (**线性可分支持向量机**)

   ​	给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习到的分离超平面为：$w^*x+b^*=0$

   ​	以及相应的分类决策函数：$f(x)=sign(w^*x+b^*)$

   称为**线性可分支持向量机**。

2. 函数间隔和几何间隔

   `定义` (函数间隔)

   ​	对于给定的训练数据集T和超平面$(w,b)$,定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为：$\hat \gamma_i=y_i(wx_i+b)$

   ​	定义超平面$(w,b)$关于训练数据集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔之最小值,即：$\hat \gamma=min_{i=1,...,N}\, \hat \gamma_i$

   

   `定义` (几何间隔)

   ​	对于给定的训练数据集T和超平面$(w,b)$,定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：$\gamma_i=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})$

   ​	定义超平面$(w,b)$关于训练数据集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的几何间隔之最小值,即：$\gamma=min_{i=1,...,N}\, \gamma_i$

3. 间隔最大化

   如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面。这个问题可以表示为下面的约束最优化问题：
   $$
   \begin{align}
   max_{w,b}\quad & \gamma\\
   s.t.\quad &y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq\gamma,i=1,2,...N
   \end{align}
   $$
   考虑几何间隔和函数间隔的关系，可改写为：
   $$
   \begin{align}
   max_{w,b}\quad &\frac{\hat\gamma}{||w||}\\
   s.t.\quad &y_i(wx_i+b)\geq \hat\gamma
   \end{align}
   $$
   取$\hat \gamma=1$,带入得：
   $$
   \begin{align}
   min_{w,b}\quad &\frac{1}{2}||w||^2\\
   s.t.\quad &y_i(wx_i+b)-1 \geq 0,i=1,2,...,N
   \end{align}
   $$

   <hr/>

   `算法` (线性可分支持向量机学习算法—最大间隔算法)

   输入：线性可分训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中，$x_i\in \chi=R^n,y_i\in\mathcal Y=\{-1，+1\}，i=1,2,...,N$;

   输出：最大间隔分离超平面和分类决策函数。

   1. 构造并求解约束最优化问题：
      $$
      \begin{align}
      min_{w,b}\quad &\frac{1}{2}||w||^2\\
      s.t.\quad &y_i(wx_i+b)-1\geq 0,i=1,2,...,N
      \end{align}
      $$
      求得最优解$w^*,b^*$

   2. 由此得到分离超平面：

      $w^*x+b^*=0$

      分类决策函数：

      $f(x)=sign(w^*x+b^*)$

4. 学习得对偶算法

   `算法` (线性可分支持向量机学习算法—对偶算法)

   输入：线性可分训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中，$x_i\in \chi=R^n,y_i\in\mathcal Y=\{-1，+1\}，i=1,2,...,N$;

   输出：最大间隔分离超平面和分类决策函数。

   1. 构造并求解约束最优化问题
      $$
      \begin{align}
      min_{\alpha}\quad &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i\\
      s.t.\quad &\sum_{i=1}^N\alpha_iy_i=0\\
      & \alpha_i\geq 0,i=1,2,...,N
      \end{align}
      $$
      求得最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$

   2. 计算

      $w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$

      并选择$\alpha^*$得一个正分量$\alpha_j^*>0$,就算：

      $b^*=y_j-\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j)$

   3. 由此得到分离超平面：

      $w^*x+b^*=0$

      分类决策函数：

      $f(x)=sign(w^*x+b^*)$

### 7.2 线性支持向量机与软间隔最大化

 1. 线性支持向量机

    ​	线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1得约束条件。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i\geq 0$,使函数间隔加上松弛变量大于等于1.这样，约束条件变为：$y_i(w\cdot x_i+b)\geq 1-\xi_i$,同时，对每个松弛变量$\xi_i$,支付一个代价$\xi_i$.目标函数变成$\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$.
    $$
    \begin{align}
    min_{w,b,\xi}\quad &\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\\
    s.t.\quad &y_i(w\cdot x_i+b)\geq 1-\xi_i,i=1,2,...,N\\
    &\xi_i\geq 0,i=1,2,...,N
    
    \end{align}
    $$
    这里C>0称为惩罚参数，一般由问题决定，C值大时对误分类的惩罚增加，C值小时对误分类的惩罚减小。

 2. 对偶算法

    `算法` (线性可分支持向量机学习算法—对偶算法)

    输入：线性可分训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,其中，$x_i\in \chi=R^n,y_i\in\mathcal Y=\{-1，+1\}，i=1,2,...,N$;

    输出：最大间隔分离超平面和分类决策函数。

    1. 构造并求解约束最优化问题
       $$
       \begin{align}min_{\alpha}\quad &\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i\\s.t.\quad &\sum_{i=1}^N\alpha_iy_i=0\\
       & 0\leq \alpha_i\leq C,i=1,2,...,N\end{align}
       $$
       求得最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$

    2. 计算

       $w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$

       并选择$\alpha^*$的适合条件的值，$0\leq \alpha_j^*\leq C$,就算：

       $b^*=y_j-\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j)$

    3. 由此得到分离超平面：

       $w^*x+b^*=0$

       分类决策函数：

       $f(x)=sign(w^*x+b^*)$

	3. 合页损失函数

    线性支持向量机学习的另外一种解释，就是最小化以下目标函数：

    ​			$\sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_+\lambda||w||^2$

    目标函数的第一项是经验损失函数或经验风险，函数

    ​                $L(y(w\cdot +b))=[1-y(x\cdot x+b)]_+$	

    称为合页损失函数。
    $$
    [z]_+=\begin{cases}
    z,\quad z>0\\
    0,\quad z\leq 0
    \end{cases}
    $$

### 7.3 非线性支持向量机与核函数

1. 核技巧

   `定义` (**核函数**)

   ​	设$\mathcal X$是输入空间，又设$\mathcal H$为特征空间，如果存在一个从$\mathcal X到\mathcal H$的映射

   ​			$\phi(x):\mathcal X\rightarrow\mathcal H$

   ​	使得对所有$x,z\in \mathcal X$,函数$K(X,Z)$满足条件：

   ​			$K(x,z)=\phi(x)\cdot\phi(z)$

   ​	则称$K(x,z)$为核函数，$\phi(x)$为映射函数.

   核技巧思想：

   ​	在学习与预测中只定义核函数,而不显示定义映射函数。通常，计算核函数比较容易，而通过映射函数计算和函数并不容易。

2. 核技巧在支持向量机中的应用

   对偶问题目标函数格式变为：
   $$
   W(\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i
   $$
   

3. 正定核

4. 常用核函数

   + 多项式核函数

     $K(x,z)=(x\cdot z+1)^p$   p次多项式

   + 高斯核函数

     $K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})$

   + 字符串核函数

### 7.4 序列最小最优化算法



## 第8章 提升方法



## 第9章 EM算法及其推广

> EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。
>
> EM算法的每次迭代由两步组成：E步，求期望；M步，求极大。

### 9.1 EM算法的引入

> 如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。
>
> EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法	

​	`算法` (**EM算法**)

​	输入：观测变量数据Y，隐变量数据Z，联合概率分布$P(Y,Z|\theta)$,条件分布$P(Z|Y,\theta)$

​	输出：模型参数$\theta$

1.  选择参数的初值$\theta^{(0)}$,开始迭代；

2. E步：记$\theta^{(i)}$为第i次迭代参数$\theta$的估计值，在第i+1次迭代的E步，计算：
   $$
   \begin{align}
   Q(\theta,\theta^{(i)})&=E_z[logP(Y,Z|\theta)|Y,\theta^{(i)}]\\
   &=\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{(i)})
   \end{align}
   $$
   这里，$P(Z|Y,\theta^{(i)})$是在给定观测数据Y和当前的参数估计$\theta^{(i)}$下隐变量数据Z的条件概率估计。

3. M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$,确定第i+1次迭代的参数的估计值$\theta^{(i+1)}$

   $\theta^{(i+1)}=argmax_\theta Q(\theta,\theta^{(i)})$

4. 重复2步和3步，直到收敛。

### 9.2 EM算法的收敛性

### 9.3 EM算法在高斯混合模型学习中的应用

#### 9.3.1 高斯混合模型

> `定义` (**高斯混合模型**)
>
> 高斯混合模型是指具有如下形式的概率分布模型：
> $$
> P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)
> $$
> 其中，$\alpha_k$是系数，$\alpha_k\geq 0,\sum_{k=1}^K\alpha_k=1,\phi(y|\theta_k)是高斯分布密度，\theta_k=(\mu_k,\sigma_k^2)$,
> $$
> \phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})
> $$
> 称为第k个模型。

> `算法` (**高斯混合模型参数估计的EM算法**)
>
> 输入：观测数据$y_1,y_2,...,y_N$,是高斯混合模型；
>
> 输出：高斯混合模型的参数。
>
> 1. 取参数的初始值开始迭代
>
> 2. E步：依据当前模型参数，计算分模型k对观测数据$y_j$的响应度
>    $$
>    \hat \gamma_{jk}=\frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)},k=1,2,...,K
>    $$
>
> 3. M步：计算新一轮迭代的模型参数
>    $$
>    \begin{align}
>    \hat\mu_k&=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}},k=1,2,...,K\\
>    \hat\sigma_k^2&=\frac{\sum_{j=1}^N\hat\gamma_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^N\hat\gamma_{jk}},k=1,2,...,K\\
>    \hat\alpha_k&=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N}
>    \end{align}
>    $$
>    4.重复2步和3步，直到收敛。

## 第10章 隐马尔可夫模型

> 隐马尔可夫模型是可用于标注问题的统计学习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。

### 10.1 隐马尔可夫模型的基本概念

1. 隐马尔可夫模型的定义

   `定义` (**隐马尔可夫模型**)

   ​	隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测序列的过程。

   ​	隐藏的马尔可夫链随机生成的状态的序列，称为**状态序列**；

   ​	每个状态生成一个观测，而由次产生的观测的随机序列，称为**观测序列**;

   ​	序列的每一个位置又可以看作是一个**时刻**。

   隐马尔可夫模型由**初始概率分布**、**状态转移概率分布**以及**观测概率分布**确定。

   **隐马尔可夫模型的形式定义如下：**

   ​	设Q是所有可能的状态的集合，V是所有可能的观测的集合。

   ​		$Q=\{q_1,q_2,...,q_N\},\quad V=\{v_1,v_2,...,v_M\}$

   其中，N是可能得状态数，M是可能得观测数。

   ​	I是长度为T得状态序列，O是对应的观测序列。

   ​					   $I=(i_1,i_2,...,i_T),\quad O=(o_1,o_2,...,o_T)$

   ​	A是状态转移概率矩阵：$A=[a_{ij}]_{N\times N}$,其中   

   ​                       $a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,...N;j=1,2,...,N$

   ​                      是在时刻t处于状态$q_i$的条件下再时刻t+1转移到状态$q_j$的概率。

   ​	B是观测概率矩阵：$B=[b_j(k)]_{N\times M}$,其中

   ​					  $b_j(k)=P(o_t=v_k|i_t=q_j),k=1,2,...,M;j=1,2,...,N$

   ​					  是在时刻t处于状态$q_j$的条件下生成观测$v_k$的概率。

   ​	$\pi$是初始状态概率向量：$\pi=(\pi_i)$,其中

   ​					  $\pi_i=P(i_1=q_i),i=1,2,...,N$

   ​					  是在时刻t=1处于状态$q_i$的概率。

   隐马尔可夫模型由初始状态概率向量、状态转移概率矩阵、观测概率矩阵决定，$\pi和A$决定状态序列，B决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三元符号表示，即: $\lambda=(\pi,A,B)$

2. 两个基本假设

   1. 齐次马尔可夫性假设

      ​	假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态即观测无关，也与时刻t无关。

      $P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,N$

   2. 观测独立性假设

      ​	假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。

      $P(o_t|i_{T},o_{T},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},,...,i_1,o_1)=P(o_t|i_t)$

3. 观测序列的生成过程

   `算法` (**观测序列的生成**)

   输入：隐马尔可夫模型$\lambda=(\pi,A,B)$,观测序列长度T；

   输出：观测序列$O=(o_1,O_2,...,o_T)$.

   1. 按照初始状态分布$\pi$产生状态$i_1$
   2. 令t=1
   3. 按照状态$i_t$的观测概率分布$b_{i_t}(k)$生成$o_t$
   4. 按照状态$i_t$的状态转移概率分布$\{a_{i_ti_{t+1}}\}$产生状态$i_{t+1},i_{t+1}=1,2,...,N$
   5. 令t=t+1;如果t<T,转(c);否则,终止

4. 隐马尔可夫模型的三个基本问题

   1. 概率计算问题

      给定模型和观测序列，计算在模型下观测序列出现的概率。

   2. 学习问题

      已知观测序列，估计模型参数，使得在该模型下观测序列概率最大。即用最大似然估计的方法估计参数。

   3. 预测问题，也称为解码问题

      已知模型和观测序列，求对给定观测序列条件概率最大的状态序列。即给定观测序列，求最有可能的对应的状态序列。

### 10.2 概率计算方法

1. 直接计算法

   状态序列$I=(i_1,i_2,...,i_T)$的概率是：

   ​	$P(I|\lambda)=\pi_{i_1}a_{i_1i_2}a_{i_2i_3}\cdot\cdot\cdot a_{i_{T-1}i_T}$

   对固定的状态序列I,观测序列$O=(o_1,o_2,...,o_T)$的概率是：

   ​	$P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)\cdot\cdot\cdot b_{i_T}(o_T)$

   O和I同时出现的联合概率为：
   $$
   \begin{align}
   P(O,I|\lambda)&=P(O|I,\lambda)P(I|\lambda)\\
   &=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdot\cdot
   \cdot a_{i_{T-1}i_T}b_{i_T}(o_T)
   \end{align}
   $$
   对所有可能的状态序列I求和，得到观测序列O的概率：
   $$
   \begin{align}
   P(O|\lambda)&=\sum_{I}P(O|I,\lambda)P(I|\lambda)\\
   &=\sum_{i_1,i_2,\cdot\cdot\cdot,i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdot\cdot
   \cdot a_{i_{T-1}i_T}b_{i_T}(o_T)
   \end{align}
   $$
   时间复杂度$O(TN^T)$

2. 前向算法

   `定义` (**前向概率**)

   ​	给定隐马尔可夫模型$\lambda$,定义到时刻t部分观测序列为$o_1,o_2,...,o_t$且状态为$q_i$的概率为前向概率，记作：$\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\lambda)$

   

   `算法` (**观测序列概率的前向算法**)

   输入：隐马尔可夫模型$\lambda$，观测序列O；

   输出：观测序列概率$P(O|\lambda)$

   1. 初值

      $\alpha_1(i)=\pi_ib_i(o_1),i=1,2,...,N$

   2. 递推  对t=1,2,...,T-1
      $$
      \alpha_{t+1}(i)=[\sum_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1}),i=1,2,...,N
      $$

   3. 终止
      $$
      P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)
      $$

3. 后向算法

   `定义` (**后向概率**)

   ​	给定隐马尔可夫模型，定义在时刻t状态为$q_i$的条件下，从t+1到T的部分观测序列为$o_{t+1},o_{t+1},...,o_T$的概率为后向概率，记作：

   ​	$\beta_t(i)=P(o_{t+1},o_{t+1},...,o_T|i_t=q_i,\lambda)$

   ​	

   `算法` (观测序列概率的后向算法)

   输入：隐马尔可夫模型$\lambda$，观测序列O；

   输出：观测序列概率$P(O|\lambda)$

   1.  $\beta_T(I)=1$

   2. 对t=T-1,T-2,...,1
      $$
      \beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{i+1})\beta_{t+1}(j),i=1,2,...,N
      $$
      

   3. $P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)$

### 10.3 学习算法



### 10.4 预测算法



## 第11章 条件随机场

 